{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "\n",
    "from data.mri.data_loader import get_data_loader\n",
    "from scripts.mri.model_loader import ModelLoader\n",
    "from scripts.mri.logger import Logger\n",
    "from scripts.mri.epoch import perform_epoch\n",
    "from utils.warmup import WarmupLR\n",
    "from utils.metrics import ImageMetricsEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/t/Documents/GIT/LearningRegularizationParametersForTGV\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = ArgumentParser()\n",
    "# parser.add_argument(\"--config\", dest=\"config\", type=str,\n",
    "#                     help=\"[Required] Path to the config file.\")\n",
    "# parser.add_argument(\"--output_dir\", dest=\"output_dir\", type=Path,\n",
    "#                     help=\"The output directory to store the `.pth` \" +\n",
    "#                     \"state dict file and other logs. \" +\n",
    "#                     \"If provided, overwrite the config.\")\n",
    "# parser.add_argument(\"--device\", dest=\"device\", type=str,\n",
    "#                     help=\"The device to use for training. \" +\n",
    "#                     \"If provided, overwrite the config. \" +\n",
    "#                     \"Recommend 'cuda' if GPU is available.\")\n",
    "# parser.add_argument(\"--uses_wandb\", dest=\"uses_wandb\", type=bool,\n",
    "#                     help=\"Whether to use WandB for logging. \" +\n",
    "#                     \"Default to False.\",\n",
    "#                     default=False)\n",
    "# parser.add_argument(\"--logs_local\", dest=\"logs_local\", type=bool,\n",
    "#                     help=\"Whether to log locally. \" +\n",
    "#                     \"If provided without value, save the config and \" +\n",
    "#                     \"other logs locally. \" +\n",
    "#                     \"If not provided, still save the config and \" +\n",
    "#                     \"other logs locally by default. \" +\n",
    "#                     \"Need to explicitly set to False to disable.\",\n",
    "#                     # Save the config and other logs locally helps\n",
    "#                     # make future reference easier.\n",
    "#                     # That's why the default is True.\n",
    "#                     default=True)\n",
    "# parser.add_argument(\"--savefile\", dest=\"savefile\", type=str,\n",
    "#                     help=\"The file to save the model state dict and config.\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {\n",
    "    \"config\": \"config/example_mri_tgv_config.yaml\",\n",
    "    \"output_dir\": None,\n",
    "    # \"device\": \"cpu\",\n",
    "    \"device\": \"mps\",\n",
    "    \"uses_wandb\": False,\n",
    "    \"logs_local\": True,\n",
    "    \"savefile\": \"model.pth\"\n",
    "}\n",
    "\n",
    "# Convert dict into a namespace\n",
    "args = type(\"Args\", (object,), args)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial config choice: config/example_mri_tgv_config.yaml\n",
      "Config loaded from file config/example_mri_tgv_config.yaml\n",
      "Loading model on device: cuda\n",
      "Output directory: ./tmp/mri_model_10_19-14_45\n",
      "Device choice: mps\n",
      "Norm of operator A: 1\n",
      "Norm of gradient operator nabla: 2.8284270763397217\n",
      "L: 3.0\n",
      "PDHG net device: mps\n",
      "Regularization: tgv\n",
      "Data path: tmp/mri_data\n",
      "\n",
      "\n",
      "Ground truth data shape: torch.Size([3000, 320, 320])\n",
      "min abs val of ground truth: 4.898725478597044e-07\n",
      "max abs val of ground truth: 2.58732533454895\n",
      "train_data_loader contains 3000 batches.\n",
      "\n",
      "\n",
      "Ground truth data shape: torch.Size([150, 320, 320])\n",
      "min abs val of ground truth: 4.5952956497785635e-06\n",
      "max abs val of ground truth: 1.241995096206665\n",
      "val_data_loader contains 150 batches.\n",
      "Action: train\n",
      "Save directory: ./tmp/mri_model_10_19-14_45\n",
      "Force overwrite: False\n",
      "Current epoch: 0\n",
      "Please initialize the logging options.\n",
      "File 'tmp/mri_model_10_19-14_45/train_epoch_metrics.csv' initialized.\n",
      "File 'tmp/mri_model_10_19-14_45/train_intermediate_metrics.csv' initialized.\n",
      "Metrics logging options initialized.\n",
      "Model saving options initialized.\n",
      "Action: val\n",
      "Save directory: ./tmp/mri_model_10_19-14_45\n",
      "Force overwrite: False\n",
      "Current epoch: 0\n",
      "Please initialize the logging options.\n",
      "File 'tmp/mri_model_10_19-14_45/val_epoch_metrics.csv' initialized.\n",
      "File 'tmp/mri_model_10_19-14_45/val_intermediate_metrics.csv' initialized.\n",
      "Metrics logging options initialized.\n",
      "Model saving options initialized.\n",
      "Saving config in ./tmp/mri_model_10_19-14_45...\n",
      "Config saved\n",
      "Saving config in ./tmp/mri_model_10_19-14_45...\n",
      "Saving config in ./tmp/mri_model_10_19-14_45...\n",
      "NOTE: Complex-to-real conversion method: abs\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# device = torch.device(device)\n",
    "\n",
    "def get_formatted_date(format_str=\"%m_%d-%H_%M\"):\n",
    "    formatted_date = datetime.now().strftime(format_str)\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "print(f\"Initial config choice: {args.config}\")\n",
    "model_loader = ModelLoader(\n",
    "    config_choice=args.config,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "if args.output_dir is None:\n",
    "    model_loader.config[\"log\"][\"save_dir\"] = \\\n",
    "        f\"./tmp/mri_model_{get_formatted_date()}\"\n",
    "else:\n",
    "    model_loader.config[\"log\"][\"save_dir\"] = args.output_dir\n",
    "print(f\"Output directory: {model_loader.config['log']['save_dir']}\")\n",
    "\n",
    "device = args.device\n",
    "if device is None:\n",
    "    device = model_loader.device\n",
    "else:\n",
    "    model_loader.config[\"device\"] = device\n",
    "print(f\"Device choice: {device}\")\n",
    "\n",
    "pdhg_net = model_loader.init_new_model()\n",
    "print(f\"Regularization: {pdhg_net.pdhg_solver.pdhg_algorithm}\")\n",
    "\n",
    "print(f\"Data path: {model_loader.config['data']['data_path']}\")\n",
    "training_data_loader = get_data_loader(\n",
    "    data_config=model_loader.config[\"data\"],\n",
    "    action=\"train\", dataset_type=\"dynamically_generated\", device=device)\n",
    "validation_data_loader = get_data_loader(\n",
    "    data_config=model_loader.config[\"data\"],\n",
    "    action=\"val\", dataset_type=\"dynamically_generated\", device=device)\n",
    "\n",
    "learning_rate = model_loader.config[\"train\"][\"learning_rate\"]\n",
    "\n",
    "optim = torch.optim.AdamW(\n",
    "    pdhg_net.parameters(),\n",
    "    # lr=args.lr,\n",
    "    # lr=1e-3,\n",
    "    lr=learning_rate,\n",
    "    # weight_decay=args.weight_decay\n",
    "    # weight_decay=1e-5\n",
    "    weight_decay=model_loader.config[\"train\"][\"weight_decay\"]\n",
    ")\n",
    "sched = WarmupLR(torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optim,\n",
    "        # (args.Nepochs - args.warmup) * len(training_data_loader),\n",
    "        (model_loader.config[\"train\"][\"expected_num_epochs\"] - 1) *\n",
    "        len(training_data_loader),\n",
    "        # eta_min=args.lr / 30, verbose=False\n",
    "        # eta_min=1e-3 / 30,\n",
    "        eta_min=learning_rate / 30,\n",
    "        # verbose=False\n",
    "    ),\n",
    "    # init_lr=args.lr / 30,\n",
    "    # init_lr=1e-3 / 30,\n",
    "    init_lr=learning_rate / 30,\n",
    "    # num_warmup=args.warmup * len(training_data_loader)\n",
    "    # num_warmup=1 * len(training_data_loader)\n",
    "    num_warmup=(\n",
    "        model_loader.config[\"train\"][\"warmup\"] *\n",
    "        len(training_data_loader))\n",
    ")\n",
    "\n",
    "train_logger = Logger(\n",
    "    action=\"train\",\n",
    "    config=model_loader.config,\n",
    "    force_overwrite=False\n",
    ")\n",
    "train_logger.init_metrics_logging_options()\n",
    "train_logger.init_model_saving_options(log_config=model_loader.config[\"log\"])\n",
    "\n",
    "val_logger = Logger(\n",
    "    action=\"val\",\n",
    "    config=model_loader.config,\n",
    "    force_overwrite=False\n",
    ")\n",
    "val_logger.init_metrics_logging_options()\n",
    "val_logger.init_model_saving_options(log_config=model_loader.config[\"log\"])\n",
    "\n",
    "# Store config and other logs if specified.\n",
    "if args.logs_local:\n",
    "    # Only need to log_config_local once.\n",
    "    train_logger.log_config_local(pdhg_net=pdhg_net)\n",
    "\n",
    "    train_logger.log_data_info(data_loader=training_data_loader)\n",
    "    val_logger.log_data_info(data_loader=validation_data_loader)\n",
    "\n",
    "# Initialize WandB for logging if specified.\n",
    "if args.uses_wandb:\n",
    "    train_logger.init_wandb()\n",
    "\n",
    "metrics_evaluator = ImageMetricsEvaluator(device=device)\n",
    "\n",
    "num_epochs = model_loader.config[\"train\"][\"num_epochs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47bd189631f453aa58af443811311a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39732a25ab484b36b7a113fe7a084319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t/Documents/GIT/LearningRegularizationParametersForTGV/utils/mask_funcs.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.fft.ifftshift(torch.tensor(mask)),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m pdhg_net\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m training_data_iterator \u001b[38;5;241m=\u001b[39m tqdm(training_data_loader)\n\u001b[0;32m----> 9\u001b[0m train_avg_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mperform_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdhg_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_evaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msets_tqdm_postfix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# pdhg_net.train(False)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# # torch.save(pdhg_net.state_dict(), f\"./model_state_dict_{epoch}.pt\")\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# train_logger.save_model(pdhg_net=pdhg_net, idx=epoch, is_final=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/scripts/mri/epoch.py:31\u001b[0m, in \u001b[0;36mperform_epoch\u001b[0;34m(data_iterator, model, logger, is_training, metrics_evaluator, learning_rate_scheduler, optimizer, sets_tqdm_postfix)\u001b[0m\n\u001b[1;32m     27\u001b[0m x_corrupted, x_true, \\\n\u001b[1;32m     28\u001b[0m     kdata_corrupted, undersampling_kmask \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     29\u001b[0m coil_sensitive_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m x_reconstructed, lambda_reg \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_kdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkdata_corrupted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_kmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mundersampling_kmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_corrupted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_csmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoil_sensitive_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m     38\u001b[0m     torch\u001b[38;5;241m.\u001b[39mview_as_real(x_reconstructed),\n\u001b[1;32m     39\u001b[0m     torch\u001b[38;5;241m.\u001b[39mview_as_real(x_true)\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m psnr, ssim \u001b[38;5;241m=\u001b[39m metrics_evaluator\u001b[38;5;241m.\u001b[39mcompute_torch_complex(\n\u001b[1;32m     43\u001b[0m     x\u001b[38;5;241m=\u001b[39mx_reconstructed, x_true\u001b[38;5;241m=\u001b[39mx_true)\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/networks/mri_pdhg_net.py:223\u001b[0m, in \u001b[0;36mMriPdhgNet.forward\u001b[0;34m(self, batch_kdata, batch_kmask, batch_x, batch_csmap, return_dict, tqdm_progress_bar)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdhg_solver\u001b[38;5;241m.\u001b[39mpdhg_algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    222\u001b[0m     lambda_reg \u001b[38;5;241m=\u001b[39m lambda_reg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda1_v\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 223\u001b[0m batch_x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdhg_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_kdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_kmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_csmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# theta=self.theta,\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: Allow learning theta?\u001b[39;49;00m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_progress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_x_reconstructed, lambda_reg\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/pdhg/mri_pdhg.py:47\u001b[0m, in \u001b[0;36mMriPdhgTorch.forward\u001b[0;34m(self, num_iters, lambda_reg, kdata, kmask, state, csmap, sigma, tau, theta, return_dict, device, tqdm_progress_bar)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mAH\u001b[39m(k: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding_object\u001b[38;5;241m.\u001b[39mapply_AH(k\u001b[38;5;241m=\u001b[39mk, csm\u001b[38;5;241m=\u001b[39mcsmap, mask\u001b[38;5;241m=\u001b[39mkmask)\n\u001b[0;32m---> 47\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdhg_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/pdhg/pdhg.py:57\u001b[0m, in \u001b[0;36mPdhgTorch.forward\u001b[0;34m(self, num_iters, A, AH, lambda_reg, z, state, sigma, tau, theta, return_dict, tqdm_progress_bar, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m tqdm_progress_bar(\u001b[38;5;28mrange\u001b[39m(num_iters))\n\u001b[0;32m---> 57\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pdhg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "File \u001b[0;32m~/Documents/GIT/LearningRegularizationParametersForTGV/pdhg/pdhg.py:319\u001b[0m, in \u001b[0;36mTgvPdhgTorch.run_pdhg\u001b[0;34m(self, iterator, A, AH, lambda_reg, z, state, sigma, tau, theta, return_dict, device)\u001b[0m\n\u001b[1;32m    317\u001b[0m     v_next \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m+\u001b[39m tau \u001b[38;5;241m*\u001b[39m (g\u001b[38;5;241m.\u001b[39mdiv_h_w(q) \u001b[38;5;241m+\u001b[39m p)\n\u001b[1;32m    318\u001b[0m     v_bar \u001b[38;5;241m=\u001b[39m v_next \u001b[38;5;241m+\u001b[39m theta \u001b[38;5;241m*\u001b[39m (v_next \u001b[38;5;241m-\u001b[39m v)\n\u001b[0;32m--> 319\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_next\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m#     torch.cuda.empty_cache()\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(args.Nepochs)):\n",
    "# for epoch in tqdm(range(1000)):\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # gc.collect()\n",
    "    train_logger.current_epoch = epoch\n",
    "    pdhg_net.train(True)\n",
    "\n",
    "    training_data_iterator = tqdm(training_data_loader)\n",
    "    train_avg_metrics = perform_epoch(\n",
    "        data_iterator=training_data_iterator,\n",
    "        model=pdhg_net,\n",
    "        logger=train_logger,\n",
    "        is_training=True,\n",
    "        metrics_evaluator=metrics_evaluator,\n",
    "        learning_rate_scheduler=sched,\n",
    "        optimizer=optim,\n",
    "        sets_tqdm_postfix=True)\n",
    "    break\n",
    "\n",
    "    # pdhg_net.train(False)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     val_logger.current_epoch = epoch\n",
    "\n",
    "    #     validation_data_iterator = tqdm(validation_data_loader)\n",
    "    #     val_avg_metrics = perform_epoch(\n",
    "    #         data_iterator=validation_data_iterator,\n",
    "    #         model=pdhg_net,\n",
    "    #         logger=val_logger,\n",
    "    #         is_training=False,\n",
    "    #         metrics_evaluator=metrics_evaluator,\n",
    "    #         sets_tqdm_postfix=True)\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "    # if wandb.run is not None:\n",
    "    #     wandb.log({\"epoch\": epoch+1})\n",
    "    # train_logger.log_metrics(metrics=train_avg_metrics, iter_idx=None)\n",
    "    # val_logger.log_metrics(metrics=val_avg_metrics, iter_idx=None)\n",
    "    # print(f\"Epoch {epoch+1}:\")\n",
    "    # print(\n",
    "    #     f\"TRAINING LOSS: {train_avg_metrics[0]}, \" +\n",
    "    #     f\"TRAINING PSNR: {train_avg_metrics[1]:.2f}, \" +\n",
    "    #     f\"TRAINING SSIM: {train_avg_metrics[2]:.4f}\")\n",
    "    # print(\n",
    "    #     f\"VALIDATION LOSS: {val_avg_metrics[0]}, \" +\n",
    "    #     f\"VALIDATION PSNR: {val_avg_metrics[1]:.2f}, \" +\n",
    "    #     f\"VALIDATION SSIM: {val_avg_metrics[2]:.4f}\")\n",
    "    # # torch.save(pdhg_net.state_dict(), f\"./model_state_dict_{epoch}.pt\")\n",
    "    # train_logger.save_model(pdhg_net=pdhg_net, idx=epoch, is_final=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save as cpu for cross-device compatibility.\n",
    "pdhg_net.cpu()\n",
    "train_logger.save_model(pdhg_net=pdhg_net, idx=None, is_final=True)\n",
    "if args.savefile is None:\n",
    "    args.savefile = f\"./model_{get_formatted_date()}.pt\"\n",
    "tosave = deepcopy(model_loader.config)\n",
    "tosave[\"state\"] = pdhg_net.state_dict()\n",
    "torch.save(tosave, args.savefile)\n",
    "print(f\"saved to {args.savefile}\")\n",
    "# torch.save(pdhg_net.state_dict(), \"./final_model_state_dict.pt\")\n",
    "\n",
    "# if wandb.run is not None:\n",
    "#     wandb.log_model(\n",
    "#         final_model_path, name=\"final_model_state_dict\")\n",
    "\n",
    "# if args.neptune:\n",
    "#     run.stop()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
